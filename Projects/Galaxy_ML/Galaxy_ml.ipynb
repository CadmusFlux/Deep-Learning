{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw = np.load('/storage/Galaxy_dataset/X.npy')\n",
    "y_raw = pd.read_csv('/storage/Galaxy_dataset/training_solutions_rev1.csv',nrows=5049)\n",
    "y_raw = y_raw['Class1.1']\n",
    "y_raw = y_raw.values.reshape((y_raw.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the n-dimensional feature space to 2 dimension [(n_x,m),(1,m)]. \n",
    "x_flatten = x_raw.reshape(x_raw.shape[0],-1).T\n",
    "y_flatten = y_raw.reshape(y_raw.shape[0],-1).T\n",
    "\n",
    "# Standardizing the RGB values\n",
    "x = x_flatten/255\n",
    "\n",
    "# Train-Test set\n",
    "x_train = x[:,:4500]\n",
    "x_test = x[:,4500:]\n",
    "y_train = y_flatten[:,:4500]\n",
    "y_test = y_flatten[:,4500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,y_train.shape[1]):\n",
    "    if y_train[0,i]>0.5:\n",
    "      y_train[0,i] = 1\n",
    "    else:\n",
    "      y_train[0,i] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,y_test.shape[1]):\n",
    "    if y_train[0,i]>0.5:\n",
    "      y_test[0,i] = 1\n",
    "    else:\n",
    "      y_test[0,i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0= x_train.shape[0]\n",
    "m= x_train.shape[1]\n",
    "n1 = 20\n",
    "n2 = 7\n",
    "n3 = 5\n",
    "n4 = 5\n",
    "n5 = 1\n",
    "layers_dims = (n0,n1,n2,n3,n4,n5)\n",
    "keep_prob = [1,0.86,0.86,0.86,0.86,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(layers_dims)\n",
    "# print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layers_dims):\n",
    "\n",
    "  np.random.seed(1)\n",
    "  parameters = {}\n",
    "  L = len(layers_dims)\n",
    "  \n",
    "  for l in range(1,L):\n",
    "    parameters[\"W\"+ str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])/ np.sqrt(layers_dims[l-1])\n",
    "    parameters[\"b\"+ str(l)] = np.zeros((layers_dims[l],1))\n",
    "\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_sampling (X,Y,minibatch_size,seed):\n",
    "    np.random.seed(seed)\n",
    "    m = x_train.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_x = x_train[:,permutation]\n",
    "    shuffled_y = y_train[:,permutation].reshape(1,m)\n",
    "    \n",
    "    total_batches = math.floor(m/minibatch_size)\n",
    "    for k in range(0,total_batches):\n",
    "        mini_batch_x = shuffled_x[:,k*minibatch_size:(k+1)*minibatch_size]\n",
    "        mini_batch_y = shuffled_y[:,k*minibatch_size:(k+1)*minibatch_size]\n",
    "        mini_batch = (mini_batch_x,mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "        \n",
    "    if m % minibatch_size != 0:\n",
    "        mini_batch_x = shuffled_x[:,(k+1)*minibatch_size:]\n",
    "        mini_batch_y = shuffled_y[:,(k+1)*minibatch_size:]\n",
    "        mini_batch = (mini_batch_x,mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    \n",
    "        \n",
    "    return mini_batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_init (parameters):\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "    v = {}\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters['W' + str(l+1)].shape))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters['b' + str(l+1)].shape))\n",
    "    return  v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_init(parameters):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\"+str(l+1)].shape))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\"+str(l+1)].shape))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\"+str(l+1)].shape))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\"+str(l+1)].shape))\n",
    "    return v,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters =  init_parameters(layers_dims)\n",
    "# print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x)),x\n",
    "def relu(x):\n",
    "    return x * (x > 0),x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid(parameters[\"W1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \n",
    "  \n",
    "    Z = np.dot(W,A)+b\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "  \n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_out_matrices(layers_dims, keep_prob,minibatch_Y):\n",
    "    np.random.seed(1)\n",
    "    D = {}\n",
    "    L = len(layers_dims)\n",
    "    \n",
    "\n",
    "    for l in range(L):\n",
    "        # initialize the random values for the dropout matrix\n",
    "        D[\"D\" + str(l)] = np.random.rand(layers_dims[l], minibatch_Y.shape[1])\n",
    "        # Convert it to 0/1 to shut down neurons corresponding to each element\n",
    "        D[\"D\" + str(l)] = D[\"D\" + str(l)] < keep_prob[l]\n",
    "        \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D = drop_out_matrices(layers_dims,m,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(D[\"D1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z,cache = linear_forward(x_train,parameters[\"W1\"],parameters[\"b1\"]) \n",
    "# print(Z,cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_activation(A,W,b,activation):\n",
    "\n",
    "  if activation == \"sigmoid\":\n",
    "    Z,linear_cache = linear_forward(A, W, b)\n",
    "    A,activation_cache = sigmoid(Z)\n",
    "  \n",
    "  elif activation ==\"relu\":\n",
    "    Z,linear_cache = linear_forward(A,W,b)\n",
    "    A,activation_cache = relu(Z) \n",
    "  cache = (linear_cache,activation_cache)\n",
    "\n",
    "  return A,cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A,cache = linear_forward_activation(x_train,parameters[\"W1\"],parameters[\"b1\"],activation=\"relu\")\n",
    "# print(A,cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(X,parameters,D,keep_prob):\n",
    "    caches = []\n",
    "    A = X\n",
    "    A = np.multiply(A,D[\"D0\"])\n",
    "    A = A/keep_prob[0]\n",
    "    L = len(parameters) // 2\n",
    "#     print(\"A_pre 0\",A ,\"/n\")\n",
    "#     print(\"A_post 0\",A ,\"/n\")\n",
    "    \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_forward_activation(A,parameters['W' + str(l)],\n",
    "                                             parameters['b' + str(l)], activation = \"relu\")\n",
    "#         print(\"A_pre\",A ,l,\"/n\")\n",
    "        A = np.multiply(A,D[\"D\"+str(l)])\n",
    "        A = A/keep_prob[l]\n",
    "#         print(\"A_post\",A ,l,\"/n\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    \n",
    "    AL, cache = linear_forward_activation(A, parameters['W' + str(L)],\n",
    "                                          parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    \n",
    "    caches.append(cache)\n",
    "   \n",
    "    \n",
    "  \n",
    "            \n",
    "    return AL, caches\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL,caches = linear_activation(x_train,parameters,D,keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(Y,AL):\n",
    "\n",
    "  m  = Y.shape[1]\n",
    "  cost = -np.sum(np.multiply(Y,np.log(AL))+np.multiply(1-Y,np.log(1-AL)))\n",
    "  return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost =  cost_function(y_train,AL)\n",
    "# print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(A):\n",
    "  R = 1*(A>0)\n",
    "  return R\n",
    "\n",
    "def sigmoid_backward(activation_cache):\n",
    "  A,o = sigmoid(activation_cache)\n",
    "  S = A*(1-A)\n",
    "  return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " def linear_backward(dZ,cache):\n",
    "\n",
    "  A_prev,W,b = cache\n",
    "  m = A_prev.shape[1]\n",
    "  \n",
    "  dW = 1/m*np.dot(dZ,A_prev.T)\n",
    "  db = 1/m*np.sum(dZ,axis=1,keepdims=True)\n",
    "  dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "  return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA,cache,activation):\n",
    "\n",
    "  linear_cache, activation_cache = cache\n",
    "    \n",
    "  if activation == \"relu\":\n",
    "        \n",
    "    dZ = dA*relu_backward(activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        \n",
    "  elif activation == \"sigmoid\":\n",
    "        \n",
    "    dZ = dA*sigmoid_backward(activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "      \n",
    "    \n",
    "  return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asd,sdf,fgh = linear_activation_backward(dAL,current_cac,activation = \"sigmoid\")\n",
    "# print(asd,sdf,fgh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches,D,keep_prob):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "  \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "   \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache, activation = \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = np.multiply(grads[\"dA\"+ str(L-1)],D[\"D\"+str(L-1)])\n",
    "    grads[\"dA\" + str(L-1)] = grads[\"dA\" + str(L-1)] / keep_prob[L-1]\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\") \n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        grads[\"dA\" + str(l)] = np.multiply(grads[\"dA\" + str(l)],D[\"D\" + str(l)])\n",
    "        grads[\"dA\" + str(l)] = grads[\"dA\" + str(l)] / keep_prob[l]\n",
    "    \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads = L_model_backward(AL, y_train, caches,D,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_param(parameters,parameter_grad,learning_rate):\n",
    "\n",
    "  L = len(parameters) // 2\n",
    "  for l in range(1,L):\n",
    "    parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - learning_rate*parameter_grad[\"dW\"+str(l)]\n",
    "    parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - learning_rate*parameter_grad[\"db\"+str(l)] \n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "        \n",
    "        ### START CODE HERE ### (approx. 4 lines)\n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l+1)] = beta*v[\"dW\" + str(l+1)] + (1-beta)*grads[\"dW\"+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta*v[\"db\" + str(l+1)] + (1-beta)*grads[\"db\"+str(l+1)]\n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l+1)] =  parameters[\"W\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] =  parameters[\"b\" + str(l+1)] - learning_rate * v[\"db\" + str(l+1)]\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters,grads,v,s,t,beta1,beta2,learning_rate,epsilon):\n",
    "    \n",
    "    L = len(parameters)// 2\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1-beta1)*grads[\"dW\"+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1-beta1)*grads[\"db\"+str(l+1)]\n",
    "        \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-np.power(beta1,t))\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-np.power(beta1,t))\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2 *s[\"dW\" + str(l+1)] + (1-beta2)*(grads[\"dW\"+str(l+1)])**2\n",
    "        s[\"db\" + str(l+1)] = beta2 *s[\"db\" + str(l+1)] + (1-beta2)*(grads[\"db\"+str(l+1)])**2\n",
    "        \n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-np.power(beta2,t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-np.power(beta2,t))\n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*(v_corrected[\"dW\" + str(l+1)]/(np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon))\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*(v_corrected[\"db\" + str(l+1)]/(np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon))\n",
    "        \n",
    "    return parameters,v,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims,optimizer ,learning_rate, mini_batch_size , beta ,beta1, beta2 ,epsilon,\n",
    "                  num_epochs,keep_prob, print_cost):\n",
    "   \n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    seed = 10\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    \n",
    "  \n",
    "    parameters = init_parameters(layers_dims)\n",
    "    \n",
    "    if optimizer == \"gd\":\n",
    "        pass\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = momentum_init(parameters)\n",
    "    elif optimizer ==  \"adam\":\n",
    "        v,s = adam_init(parameters)\n",
    "    \n",
    "    \n",
    "   \n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        seed = 10\n",
    "        seed = seed+1\n",
    "        minibatches =  mini_batch_sampling (X,Y,mini_batch_size,seed)\n",
    "        cost_total = 0\n",
    "        t= 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "            \n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "            D = drop_out_matrices(layers_dims,keep_prob,minibatch_Y)\n",
    "            \n",
    "            AL, caches = linear_activation(minibatch_X, parameters,D,keep_prob)\n",
    "\n",
    "            \n",
    "            cost_total +=  cost_function(minibatch_Y,AL)\n",
    "           \n",
    "\n",
    "            grads = L_model_backward(AL, minibatch_Y, caches,D,keep_prob)\n",
    "\n",
    "            if optimizer==\"gd\":\n",
    "                parameters = update_param(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters,v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1\n",
    "                parameters,v,s == update_parameters_with_adam(parameters,grads,v,s,t,beta1,beta2,learning_rate,epsilon)\n",
    "        \n",
    "        cost_avg = cost_total / m\n",
    "       \n",
    "        if print_cost and i % 1 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost_avg))\n",
    "        if print_cost and i % 1 == 0:\n",
    "            costs.append(cost_avg)\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.687717\n",
      "Cost after iteration 1: 0.684343\n",
      "Cost after iteration 2: 0.681760\n",
      "Cost after iteration 3: 0.679661\n",
      "Cost after iteration 4: 0.677732\n",
      "Cost after iteration 5: 0.676023\n",
      "Cost after iteration 6: 0.674393\n",
      "Cost after iteration 7: 0.672689\n",
      "Cost after iteration 8: 0.671387\n",
      "Cost after iteration 9: 0.670191\n",
      "Cost after iteration 10: 0.668949\n",
      "Cost after iteration 11: 0.667860\n",
      "Cost after iteration 12: 0.666831\n",
      "Cost after iteration 13: 0.665902\n",
      "Cost after iteration 14: 0.664984\n",
      "Cost after iteration 15: 0.664222\n",
      "Cost after iteration 16: 0.663547\n",
      "Cost after iteration 17: 0.662757\n",
      "Cost after iteration 18: 0.662116\n",
      "Cost after iteration 19: 0.661446\n",
      "Cost after iteration 20: 0.660776\n",
      "Cost after iteration 21: 0.660166\n",
      "Cost after iteration 22: 0.659628\n",
      "Cost after iteration 23: 0.659123\n",
      "Cost after iteration 24: 0.658622\n",
      "Cost after iteration 25: 0.658177\n",
      "Cost after iteration 26: 0.657706\n",
      "Cost after iteration 27: 0.657235\n",
      "Cost after iteration 28: 0.656809\n",
      "Cost after iteration 29: 0.656422\n",
      "Cost after iteration 30: 0.656018\n",
      "Cost after iteration 31: 0.655591\n",
      "Cost after iteration 32: 0.655273\n",
      "Cost after iteration 33: 0.654826\n",
      "Cost after iteration 34: 0.654504\n",
      "Cost after iteration 35: 0.654164\n",
      "Cost after iteration 36: 0.653779\n",
      "Cost after iteration 37: 0.653416\n",
      "Cost after iteration 38: 0.653004\n",
      "Cost after iteration 39: 0.652720\n",
      "Cost after iteration 40: 0.652369\n",
      "Cost after iteration 41: 0.652113\n",
      "Cost after iteration 42: 0.651831\n",
      "Cost after iteration 43: 0.651498\n",
      "Cost after iteration 44: 0.651227\n",
      "Cost after iteration 45: 0.650927\n",
      "Cost after iteration 46: 0.650597\n",
      "Cost after iteration 47: 0.650332\n",
      "Cost after iteration 48: 0.650014\n",
      "Cost after iteration 49: 0.649792\n",
      "Cost after iteration 50: 0.649449\n",
      "Cost after iteration 51: 0.649221\n",
      "Cost after iteration 52: 0.648934\n",
      "Cost after iteration 53: 0.648611\n",
      "Cost after iteration 54: 0.648222\n",
      "Cost after iteration 55: 0.647905\n",
      "Cost after iteration 56: 0.647523\n",
      "Cost after iteration 57: 0.647227\n",
      "Cost after iteration 58: 0.646898\n",
      "Cost after iteration 59: 0.646661\n",
      "Cost after iteration 60: 0.646371\n",
      "Cost after iteration 61: 0.646022\n",
      "Cost after iteration 62: 0.645837\n",
      "Cost after iteration 63: 0.645605\n",
      "Cost after iteration 64: 0.645240\n",
      "Cost after iteration 65: 0.645028\n",
      "Cost after iteration 66: 0.644772\n",
      "Cost after iteration 67: 0.644563\n",
      "Cost after iteration 68: 0.644255\n",
      "Cost after iteration 69: 0.644027\n",
      "Cost after iteration 70: 0.643839\n",
      "Cost after iteration 71: 0.643590\n",
      "Cost after iteration 72: 0.643288\n",
      "Cost after iteration 73: 0.643040\n",
      "Cost after iteration 74: 0.642878\n",
      "Cost after iteration 75: 0.642569\n",
      "Cost after iteration 76: 0.642334\n",
      "Cost after iteration 77: 0.642102\n",
      "Cost after iteration 78: 0.641941\n",
      "Cost after iteration 79: 0.641623\n",
      "Cost after iteration 80: 0.641441\n",
      "Cost after iteration 81: 0.641151\n",
      "Cost after iteration 82: 0.640947\n",
      "Cost after iteration 83: 0.640709\n",
      "Cost after iteration 84: 0.640469\n",
      "Cost after iteration 85: 0.640236\n",
      "Cost after iteration 86: 0.640020\n",
      "Cost after iteration 87: 0.639784\n",
      "Cost after iteration 88: 0.639565\n",
      "Cost after iteration 89: 0.639322\n",
      "Cost after iteration 90: 0.639076\n",
      "Cost after iteration 91: 0.638823\n",
      "Cost after iteration 92: 0.638625\n",
      "Cost after iteration 93: 0.638381\n",
      "Cost after iteration 94: 0.638150\n",
      "Cost after iteration 95: 0.637965\n",
      "Cost after iteration 96: 0.637677\n",
      "Cost after iteration 97: 0.637510\n",
      "Cost after iteration 98: 0.637222\n",
      "Cost after iteration 99: 0.637108\n",
      "Cost after iteration 100: 0.636854\n",
      "Cost after iteration 101: 0.636572\n",
      "Cost after iteration 102: 0.636446\n",
      "Cost after iteration 103: 0.636213\n",
      "Cost after iteration 104: 0.636007\n",
      "Cost after iteration 105: 0.635817\n",
      "Cost after iteration 106: 0.635584\n",
      "Cost after iteration 107: 0.635386\n",
      "Cost after iteration 108: 0.635183\n",
      "Cost after iteration 109: 0.634957\n",
      "Cost after iteration 110: 0.634787\n",
      "Cost after iteration 111: 0.634541\n",
      "Cost after iteration 112: 0.634346\n",
      "Cost after iteration 113: 0.634107\n",
      "Cost after iteration 114: 0.634097\n",
      "Cost after iteration 115: 0.633666\n",
      "Cost after iteration 116: 0.633664\n",
      "Cost after iteration 117: 0.633273\n",
      "Cost after iteration 118: 0.633280\n",
      "Cost after iteration 119: 0.632818\n",
      "Cost after iteration 120: 0.632582\n",
      "Cost after iteration 121: 0.632577\n",
      "Cost after iteration 122: 0.632356\n",
      "Cost after iteration 123: 0.632131\n",
      "Cost after iteration 124: 0.631689\n",
      "Cost after iteration 125: 0.631705\n",
      "Cost after iteration 126: 0.631243\n",
      "Cost after iteration 127: 0.631311\n",
      "Cost after iteration 128: 0.630837\n",
      "Cost after iteration 129: 0.630827\n",
      "Cost after iteration 130: 0.630685\n",
      "Cost after iteration 131: 0.630188\n",
      "Cost after iteration 132: 0.629970\n",
      "Cost after iteration 133: 0.630058\n",
      "Cost after iteration 134: 0.629840\n",
      "Cost after iteration 135: 0.629599\n",
      "Cost after iteration 136: 0.629385\n",
      "Cost after iteration 137: 0.629166\n",
      "Cost after iteration 138: 0.628728\n",
      "Cost after iteration 139: 0.628775\n",
      "Cost after iteration 140: 0.628576\n",
      "Cost after iteration 141: 0.628342\n",
      "Cost after iteration 142: 0.628158\n",
      "Cost after iteration 143: 0.627929\n",
      "Cost after iteration 144: 0.627723\n",
      "Cost after iteration 145: 0.627463\n",
      "Cost after iteration 146: 0.627334\n",
      "Cost after iteration 147: 0.627112\n",
      "Cost after iteration 148: 0.626920\n",
      "Cost after iteration 149: 0.626711\n",
      "Cost after iteration 150: 0.626496\n",
      "Cost after iteration 151: 0.626265\n",
      "Cost after iteration 152: 0.626086\n",
      "Cost after iteration 153: 0.625893\n",
      "Cost after iteration 154: 0.625700\n",
      "Cost after iteration 155: 0.625460\n",
      "Cost after iteration 156: 0.625246\n",
      "Cost after iteration 157: 0.624995\n",
      "Cost after iteration 158: 0.624861\n",
      "Cost after iteration 159: 0.624661\n",
      "Cost after iteration 160: 0.624470\n",
      "Cost after iteration 161: 0.624174\n",
      "Cost after iteration 162: 0.624080\n",
      "Cost after iteration 163: 0.623848\n",
      "Cost after iteration 164: 0.623643\n",
      "Cost after iteration 165: 0.623395\n",
      "Cost after iteration 166: 0.623246\n",
      "Cost after iteration 167: 0.623073\n",
      "Cost after iteration 168: 0.622858\n",
      "Cost after iteration 169: 0.622643\n",
      "Cost after iteration 170: 0.622441\n",
      "Cost after iteration 171: 0.622251\n",
      "Cost after iteration 172: 0.622072\n",
      "Cost after iteration 173: 0.621883\n",
      "Cost after iteration 174: 0.621668\n",
      "Cost after iteration 175: 0.621463\n",
      "Cost after iteration 176: 0.621244\n",
      "Cost after iteration 177: 0.620993\n",
      "Cost after iteration 178: 0.620817\n",
      "Cost after iteration 179: 0.620608\n",
      "Cost after iteration 180: 0.620404\n",
      "Cost after iteration 181: 0.620195\n",
      "Cost after iteration 182: 0.619977\n",
      "Cost after iteration 183: 0.619806\n",
      "Cost after iteration 184: 0.619580\n",
      "Cost after iteration 185: 0.619364\n",
      "Cost after iteration 186: 0.619083\n",
      "Cost after iteration 187: 0.618947\n",
      "Cost after iteration 188: 0.618740\n",
      "Cost after iteration 189: 0.618536\n",
      "Cost after iteration 190: 0.618337\n",
      "Cost after iteration 191: 0.618127\n",
      "Cost after iteration 192: 0.617894\n",
      "Cost after iteration 193: 0.617653\n",
      "Cost after iteration 194: 0.617393\n",
      "Cost after iteration 195: 0.617164\n",
      "Cost after iteration 196: 0.616992\n",
      "Cost after iteration 197: 0.616775\n",
      "Cost after iteration 198: 0.616550\n",
      "Cost after iteration 199: 0.616340\n",
      "Cost after iteration 200: 0.616100\n",
      "Cost after iteration 201: 0.615889\n",
      "Cost after iteration 202: 0.615607\n",
      "Cost after iteration 203: 0.615474\n",
      "Cost after iteration 204: 0.615240\n",
      "Cost after iteration 205: 0.615018\n",
      "Cost after iteration 206: 0.614779\n",
      "Cost after iteration 207: 0.614510\n",
      "Cost after iteration 208: 0.614303\n",
      "Cost after iteration 209: 0.614036\n",
      "Cost after iteration 210: 0.613795\n",
      "Cost after iteration 211: 0.613623\n",
      "Cost after iteration 212: 0.613377\n",
      "Cost after iteration 213: 0.613156\n",
      "Cost after iteration 214: 0.612849\n",
      "Cost after iteration 215: 0.612611\n",
      "Cost after iteration 216: 0.612479\n",
      "Cost after iteration 217: 0.612247\n",
      "Cost after iteration 218: 0.612001\n",
      "Cost after iteration 219: 0.611766\n",
      "Cost after iteration 220: 0.611511\n",
      "Cost after iteration 221: 0.611256\n",
      "Cost after iteration 222: 0.611038\n",
      "Cost after iteration 223: 0.610756\n",
      "Cost after iteration 224: 0.610582\n",
      "Cost after iteration 225: 0.610367\n",
      "Cost after iteration 226: 0.610085\n",
      "Cost after iteration 227: 0.609872\n",
      "Cost after iteration 228: 0.609623\n",
      "Cost after iteration 229: 0.609361\n",
      "Cost after iteration 230: 0.609180\n",
      "Cost after iteration 231: 0.608906\n",
      "Cost after iteration 232: 0.608708\n",
      "Cost after iteration 233: 0.608434\n",
      "Cost after iteration 234: 0.608201\n",
      "Cost after iteration 235: 0.607956\n",
      "Cost after iteration 236: 0.607721\n",
      "Cost after iteration 237: 0.607545\n",
      "Cost after iteration 238: 0.607337\n",
      "Cost after iteration 239: 0.607073\n",
      "Cost after iteration 240: 0.606824\n",
      "Cost after iteration 241: 0.606576\n",
      "Cost after iteration 242: 0.606380\n",
      "Cost after iteration 243: 0.606124\n",
      "Cost after iteration 244: 0.605938\n",
      "Cost after iteration 245: 0.605688\n",
      "Cost after iteration 246: 0.605465\n",
      "Cost after iteration 247: 0.605311\n",
      "Cost after iteration 248: 0.605039\n",
      "Cost after iteration 249: 0.604793\n",
      "Cost after iteration 250: 0.604594\n",
      "Cost after iteration 251: 0.604378\n",
      "Cost after iteration 252: 0.604155\n",
      "Cost after iteration 253: 0.603924\n",
      "Cost after iteration 254: 0.603703\n",
      "Cost after iteration 255: 0.603441\n",
      "Cost after iteration 256: 0.603218\n",
      "Cost after iteration 257: 0.602990\n",
      "Cost after iteration 258: 0.602762\n",
      "Cost after iteration 259: 0.602526\n",
      "Cost after iteration 260: 0.602344\n",
      "Cost after iteration 261: 0.602062\n",
      "Cost after iteration 262: 0.601926\n",
      "Cost after iteration 263: 0.601613\n",
      "Cost after iteration 264: 0.601427\n",
      "Cost after iteration 265: 0.601238\n",
      "Cost after iteration 266: 0.600996\n",
      "Cost after iteration 267: 0.600713\n",
      "Cost after iteration 268: 0.600546\n",
      "Cost after iteration 269: 0.600362\n",
      "Cost after iteration 270: 0.600046\n",
      "Cost after iteration 271: 0.599823\n",
      "Cost after iteration 272: 0.599628\n",
      "Cost after iteration 273: 0.599462\n",
      "Cost after iteration 274: 0.599228\n",
      "Cost after iteration 275: 0.598981\n",
      "Cost after iteration 276: 0.598778\n",
      "Cost after iteration 277: 0.598550\n",
      "Cost after iteration 278: 0.598323\n",
      "Cost after iteration 279: 0.598121\n",
      "Cost after iteration 280: 0.597888\n",
      "Cost after iteration 281: 0.597704\n",
      "Cost after iteration 282: 0.597492\n",
      "Cost after iteration 283: 0.597269\n",
      "Cost after iteration 284: 0.597063\n",
      "Cost after iteration 285: 0.596842\n",
      "Cost after iteration 286: 0.596617\n",
      "Cost after iteration 287: 0.596420\n",
      "Cost after iteration 288: 0.596187\n",
      "Cost after iteration 289: 0.595960\n",
      "Cost after iteration 290: 0.595756\n",
      "Cost after iteration 291: 0.595557\n",
      "Cost after iteration 292: 0.595320\n",
      "Cost after iteration 293: 0.595093\n",
      "Cost after iteration 294: 0.594874\n",
      "Cost after iteration 295: 0.594647\n",
      "Cost after iteration 296: 0.594398\n",
      "Cost after iteration 297: 0.594182\n",
      "Cost after iteration 298: 0.593995\n",
      "Cost after iteration 299: 0.593733\n",
      "Cost after iteration 300: 0.593546\n",
      "Cost after iteration 301: 0.593362\n",
      "Cost after iteration 302: 0.593100\n",
      "Cost after iteration 303: 0.592904\n",
      "Cost after iteration 304: 0.592734\n",
      "Cost after iteration 305: 0.592499\n",
      "Cost after iteration 306: 0.592318\n",
      "Cost after iteration 307: 0.592144\n",
      "Cost after iteration 308: 0.591905\n",
      "Cost after iteration 309: 0.591701\n",
      "Cost after iteration 310: 0.591499\n",
      "Cost after iteration 311: 0.591339\n",
      "Cost after iteration 312: 0.591121\n",
      "Cost after iteration 313: 0.590915\n",
      "Cost after iteration 314: 0.590710\n",
      "Cost after iteration 315: 0.590484\n",
      "Cost after iteration 316: 0.590282\n",
      "Cost after iteration 317: 0.590057\n",
      "Cost after iteration 318: 0.589878\n",
      "Cost after iteration 319: 0.589590\n",
      "Cost after iteration 320: 0.589415\n",
      "Cost after iteration 321: 0.589190\n",
      "Cost after iteration 322: 0.589001\n",
      "Cost after iteration 323: 0.588783\n",
      "Cost after iteration 324: 0.588559\n",
      "Cost after iteration 325: 0.588385\n",
      "Cost after iteration 326: 0.588176\n",
      "Cost after iteration 327: 0.587964\n",
      "Cost after iteration 328: 0.587792\n",
      "Cost after iteration 329: 0.587590\n",
      "Cost after iteration 330: 0.587376\n",
      "Cost after iteration 331: 0.587137\n",
      "Cost after iteration 332: 0.586970\n",
      "Cost after iteration 333: 0.586753\n",
      "Cost after iteration 334: 0.586559\n",
      "Cost after iteration 335: 0.586340\n",
      "Cost after iteration 336: 0.586141\n",
      "Cost after iteration 337: 0.585959\n",
      "Cost after iteration 338: 0.585723\n",
      "Cost after iteration 339: 0.585510\n",
      "Cost after iteration 340: 0.585316\n",
      "Cost after iteration 341: 0.585146\n",
      "Cost after iteration 342: 0.584932\n",
      "Cost after iteration 343: 0.584732\n",
      "Cost after iteration 344: 0.584497\n",
      "Cost after iteration 345: 0.584282\n",
      "Cost after iteration 346: 0.584137\n",
      "Cost after iteration 347: 0.583948\n",
      "Cost after iteration 348: 0.583656\n",
      "Cost after iteration 349: 0.583510\n",
      "Cost after iteration 350: 0.583336\n",
      "Cost after iteration 351: 0.583129\n",
      "Cost after iteration 352: 0.582972\n",
      "Cost after iteration 353: 0.582785\n",
      "Cost after iteration 354: 0.582517\n",
      "Cost after iteration 355: 0.582376\n",
      "Cost after iteration 356: 0.582145\n",
      "Cost after iteration 357: 0.581955\n",
      "Cost after iteration 358: 0.581758\n",
      "Cost after iteration 359: 0.581593\n",
      "Cost after iteration 360: 0.581376\n",
      "Cost after iteration 361: 0.581151\n",
      "Cost after iteration 362: 0.581016\n",
      "Cost after iteration 363: 0.580803\n",
      "Cost after iteration 364: 0.580524\n",
      "Cost after iteration 365: 0.580391\n",
      "Cost after iteration 366: 0.580252\n",
      "Cost after iteration 367: 0.580029\n",
      "Cost after iteration 368: 0.579823\n",
      "Cost after iteration 369: 0.579624\n",
      "Cost after iteration 370: 0.579450\n",
      "Cost after iteration 371: 0.579217\n",
      "Cost after iteration 372: 0.579048\n",
      "Cost after iteration 373: 0.578834\n",
      "Cost after iteration 374: 0.578678\n",
      "Cost after iteration 375: 0.578530\n",
      "Cost after iteration 376: 0.578283\n",
      "Cost after iteration 377: 0.578101\n",
      "Cost after iteration 378: 0.577935\n",
      "Cost after iteration 379: 0.577701\n",
      "Cost after iteration 380: 0.577530\n",
      "Cost after iteration 381: 0.577349\n",
      "Cost after iteration 382: 0.577157\n",
      "Cost after iteration 383: 0.577002\n",
      "Cost after iteration 384: 0.576803\n",
      "Cost after iteration 385: 0.576647\n",
      "Cost after iteration 386: 0.576388\n",
      "Cost after iteration 387: 0.576255\n",
      "Cost after iteration 388: 0.576056\n",
      "Cost after iteration 389: 0.575847\n",
      "Cost after iteration 390: 0.575687\n",
      "Cost after iteration 391: 0.575519\n",
      "Cost after iteration 392: 0.575336\n",
      "Cost after iteration 393: 0.575170\n",
      "Cost after iteration 394: 0.574884\n",
      "Cost after iteration 395: 0.574808\n",
      "Cost after iteration 396: 0.574576\n",
      "Cost after iteration 397: 0.574371\n",
      "Cost after iteration 398: 0.574183\n",
      "Cost after iteration 399: 0.574000\n",
      "Cost after iteration 400: 0.573860\n",
      "Cost after iteration 401: 0.573688\n",
      "Cost after iteration 402: 0.573494\n",
      "Cost after iteration 403: 0.573346\n",
      "Cost after iteration 404: 0.573130\n",
      "Cost after iteration 405: 0.573002\n",
      "Cost after iteration 406: 0.572757\n",
      "Cost after iteration 407: 0.572545\n",
      "Cost after iteration 408: 0.572353\n",
      "Cost after iteration 409: 0.572190\n",
      "Cost after iteration 410: 0.572001\n",
      "Cost after iteration 411: 0.571843\n",
      "Cost after iteration 412: 0.571686\n",
      "Cost after iteration 413: 0.571502\n",
      "Cost after iteration 414: 0.571248\n",
      "Cost after iteration 415: 0.571119\n",
      "Cost after iteration 416: 0.570863\n",
      "Cost after iteration 417: 0.570738\n",
      "Cost after iteration 418: 0.570543\n",
      "Cost after iteration 419: 0.570327\n",
      "Cost after iteration 420: 0.570168\n",
      "Cost after iteration 421: 0.569962\n",
      "Cost after iteration 422: 0.569801\n",
      "Cost after iteration 423: 0.569607\n",
      "Cost after iteration 424: 0.569440\n",
      "Cost after iteration 425: 0.569240\n",
      "Cost after iteration 426: 0.568978\n",
      "Cost after iteration 427: 0.568870\n",
      "Cost after iteration 428: 0.568683\n",
      "Cost after iteration 429: 0.568498\n",
      "Cost after iteration 430: 0.568264\n",
      "Cost after iteration 431: 0.568135\n",
      "Cost after iteration 432: 0.567943\n",
      "Cost after iteration 433: 0.567785\n",
      "Cost after iteration 434: 0.567692\n",
      "Cost after iteration 435: 0.567347\n",
      "Cost after iteration 436: 0.567306\n",
      "Cost after iteration 437: 0.567092\n",
      "Cost after iteration 438: 0.566764\n",
      "Cost after iteration 439: 0.566637\n",
      "Cost after iteration 440: 0.566538\n",
      "Cost after iteration 441: 0.566303\n",
      "Cost after iteration 442: 0.566151\n",
      "Cost after iteration 443: 0.565979\n",
      "Cost after iteration 444: 0.565829\n",
      "Cost after iteration 445: 0.565631\n",
      "Cost after iteration 446: 0.565509\n",
      "Cost after iteration 447: 0.565253\n",
      "Cost after iteration 448: 0.565034\n",
      "Cost after iteration 449: 0.564946\n",
      "Cost after iteration 450: 0.564722\n",
      "Cost after iteration 451: 0.564667\n",
      "Cost after iteration 452: 0.564440\n",
      "Cost after iteration 453: 0.564164\n",
      "Cost after iteration 454: 0.564031\n",
      "Cost after iteration 455: 0.563826\n",
      "Cost after iteration 456: 0.563685\n",
      "Cost after iteration 457: 0.563487\n",
      "Cost after iteration 458: 0.563328\n",
      "Cost after iteration 459: 0.563049\n",
      "Cost after iteration 460: 0.562917\n",
      "Cost after iteration 461: 0.562795\n",
      "Cost after iteration 462: 0.562587\n",
      "Cost after iteration 463: 0.562278\n",
      "Cost after iteration 464: 0.562251\n",
      "Cost after iteration 465: 0.562036\n",
      "Cost after iteration 466: 0.561908\n",
      "Cost after iteration 467: 0.561651\n",
      "Cost after iteration 468: 0.561488\n",
      "Cost after iteration 469: 0.561270\n",
      "Cost after iteration 470: 0.561154\n",
      "Cost after iteration 471: 0.560928\n",
      "Cost after iteration 472: 0.560815\n",
      "Cost after iteration 473: 0.560515\n",
      "Cost after iteration 474: 0.560474\n",
      "Cost after iteration 475: 0.560264\n",
      "Cost after iteration 476: 0.560013\n",
      "Cost after iteration 477: 0.559853\n",
      "Cost after iteration 478: 0.559645\n",
      "Cost after iteration 479: 0.559433\n",
      "Cost after iteration 480: 0.559381\n",
      "Cost after iteration 481: 0.559047\n",
      "Cost after iteration 482: 0.559000\n",
      "Cost after iteration 483: 0.558757\n",
      "Cost after iteration 484: 0.558584\n",
      "Cost after iteration 485: 0.558339\n",
      "Cost after iteration 486: 0.558297\n",
      "Cost after iteration 487: 0.558009\n",
      "Cost after iteration 488: 0.557686\n",
      "Cost after iteration 489: 0.557708\n",
      "Cost after iteration 490: 0.557448\n",
      "Cost after iteration 491: 0.557231\n",
      "Cost after iteration 492: 0.557121\n",
      "Cost after iteration 493: 0.556969\n",
      "Cost after iteration 494: 0.556775\n",
      "Cost after iteration 495: 0.556646\n",
      "Cost after iteration 496: 0.556409\n",
      "Cost after iteration 497: 0.556235\n",
      "Cost after iteration 498: 0.555989\n",
      "Cost after iteration 499: 0.555805\n",
      "Cost after iteration 500: 0.555828\n",
      "Cost after iteration 501: 0.555557\n",
      "Cost after iteration 502: 0.555488\n",
      "Cost after iteration 503: 0.555284\n",
      "Cost after iteration 504: 0.554921\n",
      "Cost after iteration 505: 0.554823\n",
      "Cost after iteration 506: 0.554710\n",
      "Cost after iteration 507: 0.554563\n",
      "Cost after iteration 508: 0.554307\n",
      "Cost after iteration 509: 0.554136\n",
      "Cost after iteration 510: 0.553963\n",
      "Cost after iteration 511: 0.553756\n",
      "Cost after iteration 512: 0.553407\n",
      "Cost after iteration 513: 0.553402\n",
      "Cost after iteration 514: 0.553238\n",
      "Cost after iteration 515: 0.553075\n",
      "Cost after iteration 516: 0.552873\n",
      "Cost after iteration 517: 0.552709\n",
      "Cost after iteration 518: 0.552537\n",
      "Cost after iteration 519: 0.552357\n",
      "Cost after iteration 520: 0.552145\n",
      "Cost after iteration 521: 0.551961\n",
      "Cost after iteration 522: 0.551804\n",
      "Cost after iteration 523: 0.551633\n",
      "Cost after iteration 524: 0.551466\n",
      "Cost after iteration 525: 0.551105\n",
      "Cost after iteration 526: 0.551118\n",
      "Cost after iteration 527: 0.551003\n",
      "Cost after iteration 528: 0.550646\n",
      "Cost after iteration 529: 0.550566\n",
      "Cost after iteration 530: 0.550289\n",
      "Cost after iteration 531: 0.550117\n",
      "Cost after iteration 532: 0.549958\n",
      "Cost after iteration 533: 0.549753\n",
      "Cost after iteration 534: 0.549540\n",
      "Cost after iteration 535: 0.549492\n",
      "Cost after iteration 536: 0.549136\n",
      "Cost after iteration 537: 0.549004\n",
      "Cost after iteration 538: 0.548877\n",
      "Cost after iteration 539: 0.548702\n",
      "Cost after iteration 540: 0.548420\n",
      "Cost after iteration 541: 0.548234\n",
      "Cost after iteration 542: 0.548287\n",
      "Cost after iteration 543: 0.547878\n",
      "Cost after iteration 544: 0.547744\n",
      "Cost after iteration 545: 0.547696\n",
      "Cost after iteration 546: 0.547504\n",
      "Cost after iteration 547: 0.547294\n",
      "Cost after iteration 548: 0.546941\n",
      "Cost after iteration 549: 0.546848\n",
      "Cost after iteration 550: 0.546678\n",
      "Cost after iteration 551: 0.546640\n",
      "Cost after iteration 552: 0.546545\n",
      "Cost after iteration 553: 0.546183\n",
      "Cost after iteration 554: 0.546087\n",
      "Cost after iteration 555: 0.545840\n",
      "Cost after iteration 556: 0.545825\n",
      "Cost after iteration 557: 0.545420\n",
      "Cost after iteration 558: 0.545505\n",
      "Cost after iteration 559: 0.545151\n",
      "Cost after iteration 560: 0.544821\n",
      "Cost after iteration 561: 0.544647\n",
      "Cost after iteration 562: 0.544532\n",
      "Cost after iteration 563: 0.544386\n",
      "Cost after iteration 564: 0.544142\n",
      "Cost after iteration 565: 0.544157\n",
      "Cost after iteration 566: 0.543906\n",
      "Cost after iteration 567: 0.543666\n",
      "Cost after iteration 568: 0.543523\n",
      "Cost after iteration 569: 0.543552\n",
      "Cost after iteration 570: 0.543257\n",
      "Cost after iteration 571: 0.543024\n",
      "Cost after iteration 572: 0.542872\n",
      "Cost after iteration 573: 0.542573\n",
      "Cost after iteration 574: 0.542484\n",
      "Cost after iteration 575: 0.542315\n",
      "Cost after iteration 576: 0.542040\n",
      "Cost after iteration 577: 0.541859\n",
      "Cost after iteration 578: 0.541718\n",
      "Cost after iteration 579: 0.541575\n",
      "Cost after iteration 580: 0.541286\n",
      "Cost after iteration 581: 0.541295\n",
      "Cost after iteration 582: 0.541126\n",
      "Cost after iteration 583: 0.540914\n",
      "Cost after iteration 584: 0.540439\n",
      "Cost after iteration 585: 0.540337\n",
      "Cost after iteration 586: 0.540230\n",
      "Cost after iteration 587: 0.540080\n",
      "Cost after iteration 588: 0.539952\n",
      "Cost after iteration 589: 0.539750\n",
      "Cost after iteration 590: 0.539512\n",
      "Cost after iteration 591: 0.539455\n",
      "Cost after iteration 592: 0.539167\n",
      "Cost after iteration 593: 0.538984\n",
      "Cost after iteration 594: 0.538778\n",
      "Cost after iteration 595: 0.538600\n",
      "Cost after iteration 596: 0.538658\n",
      "Cost after iteration 597: 0.538224\n",
      "Cost after iteration 598: 0.538048\n",
      "Cost after iteration 599: 0.537855\n",
      "Cost after iteration 600: 0.537731\n",
      "Cost after iteration 601: 0.537623\n",
      "Cost after iteration 602: 0.537458\n",
      "Cost after iteration 603: 0.537238\n",
      "Cost after iteration 604: 0.537092\n",
      "Cost after iteration 605: 0.536691\n",
      "Cost after iteration 606: 0.536493\n",
      "Cost after iteration 607: 0.536546\n",
      "Cost after iteration 608: 0.536262\n",
      "Cost after iteration 609: 0.535998\n",
      "Cost after iteration 610: 0.536008\n",
      "Cost after iteration 611: 0.535670\n",
      "Cost after iteration 612: 0.535673\n",
      "Cost after iteration 613: 0.535161\n",
      "Cost after iteration 614: 0.535204\n",
      "Cost after iteration 615: 0.534888\n",
      "Cost after iteration 616: 0.534789\n",
      "Cost after iteration 617: 0.534844\n",
      "Cost after iteration 618: 0.534420\n",
      "Cost after iteration 619: 0.534420\n",
      "Cost after iteration 620: 0.533981\n",
      "Cost after iteration 621: 0.533789\n",
      "Cost after iteration 622: 0.533863\n",
      "Cost after iteration 623: 0.533462\n",
      "Cost after iteration 624: 0.533435\n",
      "Cost after iteration 625: 0.533182\n",
      "Cost after iteration 626: 0.532968\n",
      "Cost after iteration 627: 0.532715\n",
      "Cost after iteration 628: 0.532733\n",
      "Cost after iteration 629: 0.532377\n",
      "Cost after iteration 630: 0.532217\n",
      "Cost after iteration 631: 0.532076\n",
      "Cost after iteration 632: 0.531967\n",
      "Cost after iteration 633: 0.531593\n",
      "Cost after iteration 634: 0.531467\n",
      "Cost after iteration 635: 0.531564\n",
      "Cost after iteration 636: 0.531412\n",
      "Cost after iteration 637: 0.531194\n",
      "Cost after iteration 638: 0.531073\n",
      "Cost after iteration 639: 0.530857\n",
      "Cost after iteration 640: 0.530361\n",
      "Cost after iteration 641: 0.530239\n",
      "Cost after iteration 642: 0.530228\n",
      "Cost after iteration 643: 0.529806\n",
      "Cost after iteration 644: 0.529881\n",
      "Cost after iteration 645: 0.529626\n",
      "Cost after iteration 646: 0.529576\n",
      "Cost after iteration 647: 0.529178\n",
      "Cost after iteration 648: 0.528773\n",
      "Cost after iteration 649: 0.528822\n",
      "Cost after iteration 650: 0.528783\n",
      "Cost after iteration 651: 0.528568\n",
      "Cost after iteration 652: 0.528052\n",
      "Cost after iteration 653: 0.527966\n",
      "Cost after iteration 654: 0.527940\n",
      "Cost after iteration 655: 0.527662\n",
      "Cost after iteration 656: 0.527372\n",
      "Cost after iteration 657: 0.527292\n",
      "Cost after iteration 658: 0.527023\n",
      "Cost after iteration 659: 0.526908\n",
      "Cost after iteration 660: 0.526614\n",
      "Cost after iteration 661: 0.526571\n",
      "Cost after iteration 662: 0.526587\n",
      "Cost after iteration 663: 0.526369\n",
      "Cost after iteration 664: 0.526016\n",
      "Cost after iteration 665: 0.525751\n",
      "Cost after iteration 666: 0.525780\n",
      "Cost after iteration 667: 0.525691\n",
      "Cost after iteration 668: 0.525407\n",
      "Cost after iteration 669: 0.525238\n",
      "Cost after iteration 670: 0.524721\n",
      "Cost after iteration 671: 0.524753\n",
      "Cost after iteration 672: 0.524694\n",
      "Cost after iteration 673: 0.524265\n",
      "Cost after iteration 674: 0.524367\n",
      "Cost after iteration 675: 0.523914\n",
      "Cost after iteration 676: 0.524133\n",
      "Cost after iteration 677: 0.523796\n",
      "Cost after iteration 678: 0.523675\n",
      "Cost after iteration 679: 0.523314\n",
      "Cost after iteration 680: 0.523552\n",
      "Cost after iteration 681: 0.523098\n",
      "Cost after iteration 682: 0.522871\n",
      "Cost after iteration 683: 0.522428\n",
      "Cost after iteration 684: 0.522619\n",
      "Cost after iteration 685: 0.522077\n",
      "Cost after iteration 686: 0.522044\n",
      "Cost after iteration 687: 0.522041\n",
      "Cost after iteration 688: 0.521622\n",
      "Cost after iteration 689: 0.521921\n",
      "Cost after iteration 690: 0.521551\n",
      "Cost after iteration 691: 0.521400\n",
      "Cost after iteration 692: 0.521294\n",
      "Cost after iteration 693: 0.520584\n",
      "Cost after iteration 694: 0.520606\n",
      "Cost after iteration 695: 0.520963\n",
      "Cost after iteration 696: 0.520091\n",
      "Cost after iteration 697: 0.519983\n",
      "Cost after iteration 698: 0.519784\n",
      "Cost after iteration 699: 0.519798\n",
      "Cost after iteration 700: 0.519204\n",
      "Cost after iteration 701: 0.518924\n",
      "Cost after iteration 702: 0.518903\n",
      "Cost after iteration 703: 0.518978\n",
      "Cost after iteration 704: 0.518962\n",
      "Cost after iteration 705: 0.518262\n",
      "Cost after iteration 706: 0.518340\n",
      "Cost after iteration 707: 0.517823\n",
      "Cost after iteration 708: 0.517651\n",
      "Cost after iteration 709: 0.517475\n",
      "Cost after iteration 710: 0.517468\n",
      "Cost after iteration 711: 0.517133\n",
      "Cost after iteration 712: 0.517512\n",
      "Cost after iteration 713: 0.516536\n",
      "Cost after iteration 714: 0.517072\n",
      "Cost after iteration 715: 0.517041\n",
      "Cost after iteration 716: 0.516750\n",
      "Cost after iteration 717: 0.516750\n",
      "Cost after iteration 718: 0.516461\n",
      "Cost after iteration 719: 0.515735\n",
      "Cost after iteration 720: 0.515256\n",
      "Cost after iteration 721: 0.514946\n",
      "Cost after iteration 722: 0.515515\n",
      "Cost after iteration 723: 0.515508\n",
      "Cost after iteration 724: 0.514682\n",
      "Cost after iteration 725: 0.514466\n",
      "Cost after iteration 726: 0.514924\n",
      "Cost after iteration 727: 0.514586\n",
      "Cost after iteration 728: 0.513643\n",
      "Cost after iteration 729: 0.513920\n",
      "Cost after iteration 730: 0.513596\n",
      "Cost after iteration 731: 0.513385\n",
      "Cost after iteration 732: 0.513558\n",
      "Cost after iteration 733: 0.513118\n",
      "Cost after iteration 734: 0.512430\n",
      "Cost after iteration 735: 0.512967\n",
      "Cost after iteration 736: 0.512581\n",
      "Cost after iteration 737: 0.511992\n",
      "Cost after iteration 738: 0.512995\n",
      "Cost after iteration 739: 0.512400\n",
      "Cost after iteration 740: 0.511728\n",
      "Cost after iteration 741: 0.511195\n",
      "Cost after iteration 742: 0.511501\n",
      "Cost after iteration 743: 0.511692\n",
      "Cost after iteration 744: 0.510530\n",
      "Cost after iteration 745: 0.510716\n",
      "Cost after iteration 746: 0.510902\n",
      "Cost after iteration 747: 0.510831\n",
      "Cost after iteration 748: 0.510165\n",
      "Cost after iteration 749: 0.509762\n",
      "Cost after iteration 750: 0.510137\n",
      "Cost after iteration 751: 0.509772\n",
      "Cost after iteration 752: 0.509335\n",
      "Cost after iteration 753: 0.509570\n",
      "Cost after iteration 754: 0.509417\n",
      "Cost after iteration 755: 0.508940\n",
      "Cost after iteration 756: 0.508795\n",
      "Cost after iteration 757: 0.509157\n",
      "Cost after iteration 758: 0.508286\n",
      "Cost after iteration 759: 0.508029\n",
      "Cost after iteration 760: 0.507824\n",
      "Cost after iteration 761: 0.507858\n",
      "Cost after iteration 762: 0.507649\n",
      "Cost after iteration 763: 0.507148\n",
      "Cost after iteration 764: 0.507037\n",
      "Cost after iteration 765: 0.507458\n",
      "Cost after iteration 766: 0.507259\n",
      "Cost after iteration 767: 0.506204\n",
      "Cost after iteration 768: 0.506550\n",
      "Cost after iteration 769: 0.506462\n",
      "Cost after iteration 770: 0.505695\n",
      "Cost after iteration 771: 0.506063\n",
      "Cost after iteration 772: 0.505174\n",
      "Cost after iteration 773: 0.505459\n",
      "Cost after iteration 774: 0.505261\n",
      "Cost after iteration 775: 0.505287\n",
      "Cost after iteration 776: 0.504880\n",
      "Cost after iteration 777: 0.504339\n",
      "Cost after iteration 778: 0.504574\n",
      "Cost after iteration 779: 0.504529\n",
      "Cost after iteration 780: 0.503811\n",
      "Cost after iteration 781: 0.503742\n",
      "Cost after iteration 782: 0.503603\n",
      "Cost after iteration 783: 0.503728\n",
      "Cost after iteration 784: 0.503085\n",
      "Cost after iteration 785: 0.503683\n",
      "Cost after iteration 786: 0.503479\n",
      "Cost after iteration 787: 0.502442\n",
      "Cost after iteration 788: 0.502244\n",
      "Cost after iteration 789: 0.503086\n",
      "Cost after iteration 790: 0.502202\n",
      "Cost after iteration 791: 0.502093\n",
      "Cost after iteration 792: 0.502074\n",
      "Cost after iteration 793: 0.501301\n",
      "Cost after iteration 794: 0.502523\n",
      "Cost after iteration 795: 0.501481\n",
      "Cost after iteration 796: 0.500884\n",
      "Cost after iteration 797: 0.501578\n",
      "Cost after iteration 798: 0.501437\n",
      "Cost after iteration 799: 0.500480\n",
      "Cost after iteration 800: 0.500852\n",
      "Cost after iteration 801: 0.500569\n",
      "Cost after iteration 802: 0.499938\n",
      "Cost after iteration 803: 0.499804\n",
      "Cost after iteration 804: 0.499671\n",
      "Cost after iteration 805: 0.500281\n",
      "Cost after iteration 806: 0.498941\n",
      "Cost after iteration 807: 0.498819\n",
      "Cost after iteration 808: 0.499415\n",
      "Cost after iteration 809: 0.498747\n",
      "Cost after iteration 810: 0.498103\n",
      "Cost after iteration 811: 0.497798\n",
      "Cost after iteration 812: 0.498217\n",
      "Cost after iteration 813: 0.497606\n",
      "Cost after iteration 814: 0.498081\n",
      "Cost after iteration 815: 0.498235\n",
      "Cost after iteration 816: 0.498124\n",
      "Cost after iteration 817: 0.498283\n",
      "Cost after iteration 818: 0.496890\n",
      "Cost after iteration 819: 0.496265\n",
      "Cost after iteration 820: 0.497601\n",
      "Cost after iteration 821: 0.497208\n",
      "Cost after iteration 822: 0.496736\n",
      "Cost after iteration 823: 0.496864\n",
      "Cost after iteration 824: 0.496899\n",
      "Cost after iteration 825: 0.496011\n",
      "Cost after iteration 826: 0.496220\n",
      "Cost after iteration 827: 0.496369\n",
      "Cost after iteration 828: 0.495986\n",
      "Cost after iteration 829: 0.496033\n",
      "Cost after iteration 830: 0.494908\n",
      "Cost after iteration 831: 0.494940\n",
      "Cost after iteration 832: 0.495506\n",
      "Cost after iteration 833: 0.494961\n",
      "Cost after iteration 834: 0.493792\n",
      "Cost after iteration 835: 0.494550\n",
      "Cost after iteration 836: 0.494354\n",
      "Cost after iteration 837: 0.494467\n",
      "Cost after iteration 838: 0.494649\n",
      "Cost after iteration 839: 0.493917\n",
      "Cost after iteration 840: 0.494283\n",
      "Cost after iteration 841: 0.493964\n",
      "Cost after iteration 842: 0.492737\n",
      "Cost after iteration 843: 0.493251\n",
      "Cost after iteration 844: 0.493286\n",
      "Cost after iteration 845: 0.491873\n",
      "Cost after iteration 846: 0.493208\n",
      "Cost after iteration 847: 0.492732\n",
      "Cost after iteration 848: 0.491978\n",
      "Cost after iteration 849: 0.492378\n",
      "Cost after iteration 850: 0.492337\n",
      "Cost after iteration 851: 0.492537\n",
      "Cost after iteration 852: 0.491029\n",
      "Cost after iteration 853: 0.491117\n",
      "Cost after iteration 854: 0.491520\n",
      "Cost after iteration 855: 0.491374\n",
      "Cost after iteration 856: 0.490298\n",
      "Cost after iteration 857: 0.490780\n",
      "Cost after iteration 858: 0.490938\n",
      "Cost after iteration 859: 0.489638\n",
      "Cost after iteration 860: 0.491013\n",
      "Cost after iteration 861: 0.490301\n",
      "Cost after iteration 862: 0.489981\n",
      "Cost after iteration 863: 0.489407\n",
      "Cost after iteration 864: 0.489466\n",
      "Cost after iteration 865: 0.488919\n",
      "Cost after iteration 866: 0.488263\n",
      "Cost after iteration 867: 0.489232\n",
      "Cost after iteration 868: 0.488289\n",
      "Cost after iteration 869: 0.488792\n",
      "Cost after iteration 870: 0.487890\n",
      "Cost after iteration 871: 0.487828\n",
      "Cost after iteration 872: 0.488560\n",
      "Cost after iteration 873: 0.488107\n",
      "Cost after iteration 874: 0.487655\n",
      "Cost after iteration 875: 0.488504\n",
      "Cost after iteration 876: 0.487609\n",
      "Cost after iteration 877: 0.486879\n",
      "Cost after iteration 878: 0.487193\n",
      "Cost after iteration 879: 0.486360\n",
      "Cost after iteration 880: 0.487609\n",
      "Cost after iteration 881: 0.486088\n",
      "Cost after iteration 882: 0.485957\n",
      "Cost after iteration 883: 0.486911\n",
      "Cost after iteration 884: 0.486370\n",
      "Cost after iteration 885: 0.485853\n",
      "Cost after iteration 886: 0.485500\n",
      "Cost after iteration 887: 0.486418\n",
      "Cost after iteration 888: 0.485680\n",
      "Cost after iteration 889: 0.484739\n",
      "Cost after iteration 890: 0.486565\n",
      "Cost after iteration 891: 0.484730\n",
      "Cost after iteration 892: 0.484820\n",
      "Cost after iteration 893: 0.484253\n",
      "Cost after iteration 894: 0.483729\n",
      "Cost after iteration 895: 0.484120\n",
      "Cost after iteration 896: 0.484296\n",
      "Cost after iteration 897: 0.484202\n",
      "Cost after iteration 898: 0.483641\n",
      "Cost after iteration 899: 0.482935\n",
      "Cost after iteration 900: 0.483039\n",
      "Cost after iteration 901: 0.483064\n",
      "Cost after iteration 902: 0.482812\n",
      "Cost after iteration 903: 0.482714\n",
      "Cost after iteration 904: 0.483293\n",
      "Cost after iteration 905: 0.482257\n",
      "Cost after iteration 906: 0.482721\n",
      "Cost after iteration 907: 0.481994\n",
      "Cost after iteration 908: 0.482356\n",
      "Cost after iteration 909: 0.481485\n",
      "Cost after iteration 910: 0.482390\n",
      "Cost after iteration 911: 0.482056\n",
      "Cost after iteration 912: 0.481276\n",
      "Cost after iteration 913: 0.481055\n",
      "Cost after iteration 914: 0.480280\n",
      "Cost after iteration 915: 0.481067\n",
      "Cost after iteration 916: 0.481323\n",
      "Cost after iteration 917: 0.480250\n",
      "Cost after iteration 918: 0.480650\n",
      "Cost after iteration 919: 0.480277\n",
      "Cost after iteration 920: 0.480301\n",
      "Cost after iteration 921: 0.479810\n",
      "Cost after iteration 922: 0.479965\n",
      "Cost after iteration 923: 0.479775\n",
      "Cost after iteration 924: 0.479488\n",
      "Cost after iteration 925: 0.478980\n",
      "Cost after iteration 926: 0.479444\n",
      "Cost after iteration 927: 0.478783\n",
      "Cost after iteration 928: 0.478226\n",
      "Cost after iteration 929: 0.478284\n",
      "Cost after iteration 930: 0.479668\n",
      "Cost after iteration 931: 0.477655\n",
      "Cost after iteration 932: 0.478297\n",
      "Cost after iteration 933: 0.477411\n",
      "Cost after iteration 934: 0.477318\n",
      "Cost after iteration 935: 0.476989\n",
      "Cost after iteration 936: 0.476648\n",
      "Cost after iteration 937: 0.477452\n",
      "Cost after iteration 938: 0.477861\n",
      "Cost after iteration 939: 0.477314\n",
      "Cost after iteration 940: 0.477347\n",
      "Cost after iteration 941: 0.476429\n",
      "Cost after iteration 942: 0.476638\n",
      "Cost after iteration 943: 0.476861\n",
      "Cost after iteration 944: 0.475338\n",
      "Cost after iteration 945: 0.476366\n",
      "Cost after iteration 946: 0.474755\n"
     ]
    }
   ],
   "source": [
    "paramet = L_layer_model(x_train, y_train, layers_dims,\"gd\" ,0.0007, 128 , 0.9 ,0.9, 0.999 ,1e-8,\n",
    "                  2000,keep_prob, True)\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,y,parameters):\n",
    "  m = X.shape[1]\n",
    "  p = np.zeros((1,m))\n",
    "  yc = np.zeros((1,m))\n",
    "  n = len(parameters)// 2 \n",
    "\n",
    "  probas,caches = linear_activation(X, parameters)\n",
    "\n",
    "  for i in range(0,probas.shape[1]):\n",
    "    if probas[0,i]>0.5:\n",
    "      p[0,i] = 1\n",
    "    else:\n",
    "      p[0,i] = 0\n",
    "    if y[0,i]>0.5:\n",
    "      yc[0,i] = 1\n",
    "    else:\n",
    "      yc[0,i] = 0\n",
    "  print(\"probas\",str(probas))  \n",
    "  print(\"predictions\",str(p))\n",
    "  print(\"yc\", str(yc))\n",
    "  print(\"label\", str(y))\n",
    "  print(\"Accuracy: \"  + str(np.sum((p == yc)/m)))\n",
    "  return p;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = predict(x_train,y_train,paramet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = predict(x_test,y_test,paramet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = predict(x_test,y_test,paramet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/storage/Galaxy_dataset/parameter.npy',paramet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/storage/Galaxy_dataset/probas.npy',probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
