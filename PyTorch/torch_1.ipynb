{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-791efc54-0fe4-4316-bf98-69faba67f60c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "71515f25",
    "execution_start": 1623324649437,
    "execution_millis": 15,
    "deepnote_cell_type": "code"
   },
   "source": "from pathlib import Path\nimport requests",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "    1. Path and request module are used for fetching data from src.\n    2. The data is a zip file which contains the MNIST dataset in\n       pickle format.\n    3. Pickle format is used for serializing data(binary stream)\n       objects in python.",
   "metadata": {
    "tags": [],
    "cell_id": "00001-dc50f681-731d-4b0a-9277-c3706f13ba73",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-2c290225-39ca-4094-937c-f7aa00936624",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "af398855",
    "execution_start": 1623324649459,
    "execution_millis": 49,
    "deepnote_cell_type": "code"
   },
   "source": "DATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir( parents=True, exist_ok = True )\n\nURL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n\n    content = requests.get(URL+FILENAME).content\n    (PATH / FILENAME).open(\"wb\").write(content)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Here we use gzip and pickle module to unzip and access the dataset.",
   "metadata": {
    "tags": [],
    "cell_id": "00003-eccee77e-3a99-46f3-b010-56f339302af8",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00002-35c8ee8b-8126-4f83-b885-0daf350f1c20",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3551f4d7",
    "execution_start": 1623324649509,
    "execution_millis": 1389,
    "deepnote_cell_type": "code"
   },
   "source": "import pickle\nimport gzip\n\nwith gzip.open((PATH / FILENAME).as_posix(),\"rb\") as f:\n        ((x_train,y_train) ,(x_valid,y_valid),_) = pickle.load(f,\n            encoding= \"latin-1\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "    1. The dataset are pixels of images but they are stored as a\n       flattened single vector or array with 784(28*28) columns or\n       entries. \n    2. Inorder to plot the image we need to reshape it to 2D.\n    3. There are 50,000 images or instances.",
   "metadata": {
    "tags": [],
    "cell_id": "00005-d98b3bda-afce-44c0-af97-d5d9b30122e6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-61f5da25-f272-49fd-9aeb-1c8dfba1815f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3c530038",
    "execution_start": 1623324650900,
    "execution_millis": 1820,
    "deepnote_cell_type": "code"
   },
   "source": "from matplotlib import pyplot\nimport numpy as np\n\npyplot.imshow(x_train[0].reshape(28,28),cmap = \"gray\")\nprint(x_train.shape)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "(50000, 784)\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light",
      "image/png": {
       "width": 251,
       "height": 248
      }
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "1. PyTorch works on torch.tensor data-objects but our data is an\n   array. Hence we need to construct tensor object from our dataset.\n2. Torch.tensor is a module which does this,  hence we use map and\n   (tensor module & corresponding array objects) to replace the array\n   storing variables with tensor objects. ",
   "metadata": {
    "tags": [],
    "cell_id": "00007-e512060e-373f-4248-ad74-ef32a6fb8bdb",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-025e67c4-191d-4bfe-adbd-c06075f279e8",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d574bb60",
    "execution_start": 1623324652727,
    "execution_millis": 1101,
    "output_cleared": false,
    "deepnote_cell_type": "code"
   },
   "source": "import torch\n\nx_train , y_train , x_valid , y_valid = map(\n        torch.tensor , (x_train,y_train,x_valid,y_valid)\n)\nn,c = x_train.shape\n\nprint(x_train,y_train)\nprint(x_train.shape)\nprint(y_train.min(),y_train.max())",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\ntorch.Size([50000, 784])\ntensor(0) tensor(9)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Model (with tensor operations)",
   "metadata": {
    "tags": [],
    "cell_id": "00009-4793e7d0-789b-4302-a6b3-a19c55297a89",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Initializing weights &amp; biases",
   "metadata": {
    "tags": [],
    "cell_id": "00010-61a9826e-efeb-43fb-94f6-2aaa366271f2",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Dimensions :\n\n   1. x (feature-vector) : (64,784) (instances , features or pixels)\n   2. y (Label) : (64,1) (instances , labels)\n   3. W (weights) : (784,10) ( features or x[1] , neurons in layer)\n   4. b (bias) : (1,10)\n   5. Z (activation) : (1,10)    ",
   "metadata": {
    "tags": [],
    "cell_id": "00011-1850f757-ea84-4c19-b486-c4f3a68bfe91",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00006-8639d22c-d7f1-48e0-b5b7-6ab97a5c6436",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ad09f98e",
    "execution_start": 1623324653845,
    "execution_millis": 9,
    "deepnote_cell_type": "code"
   },
   "source": "import math\n\nweights = torch.randn(784,10) / math.sqrt(784)\nweights.requires_grad_()\nbias = torch.zeros(10,requires_grad= True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Activation Function",
   "metadata": {
    "tags": [],
    "cell_id": "00011-9fb15a9b-8aef-4d12-b8e3-e01d0b1a064b",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "1. What we're doing here is basically building a neural net for a classification problem with 10 outputs (0-9).\n2. This doesn't have hidden layers.\n3. The activation function used here is log-softmax.\n4. This provides probability of each class for a input/instance.\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00011-5eecc0a8-74c7-4cbd-97d1-fb919223b74e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00012-8639fe9d-a2f5-4a5a-b322-88af855280d0",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "95009cce",
    "execution_start": 1623324653865,
    "execution_millis": 45,
    "deepnote_cell_type": "code"
   },
   "source": "def log_softmax(x):\n    return x - x.exp().sum(-1).log().unsqueeze(-1)\n\ndef model(xb):\n    return log_softmax(xb @ weights + bias)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00012-94dd6d80-dd14-4e34-ae8d-3c45eb9a911e",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "52b41fee",
    "execution_start": 1623324653911,
    "execution_millis": 58,
    "output_cleared": false,
    "deepnote_cell_type": "code"
   },
   "source": "bs = 64\n\nxb = x_train[0:bs]\npreds = model(xb)\npreds[0] , preds.shape\nprint(preds[0],preds.shape)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "tensor([-1.9614, -2.5732, -1.9593, -2.7612, -1.8503, -1.8847, -2.3464, -2.6803,\n        -2.9172, -2.9669], grad_fn=<SelectBackward>) torch.Size([64, 10])\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Loss Function",
   "metadata": {
    "tags": [],
    "cell_id": "00017-5c5ab771-38b4-4412-93c8-cbff61ecfb3c",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "1. The dimension of our prediction is [64,10]. Which means it contains the probability of all the 10\npossible outputs.\n2. We just want to consider the probability corresponding to the actual output of the instance.Hence we use indexing on the preds(tensor) . \n3. preds(range(0,64),target) : \n    1. The first arg is for rows ,here it is range of 63 indexes. \n    2. The second arg is for columns, here we provide the target(y) itself. It contains the output of the instances which also fortunately works as index. \n    3. So for e.g, if the value in y is 5 for a instance we can look at 5th index in the preds[tensor] for that instance to get the probability of the output being 5. \n4. Lastly we take the mean of the obtained tensor , this is our loss.",
   "metadata": {
    "tags": [],
    "cell_id": "00018-f5d12e56-d5ae-4097-9334-e86b6389897b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00018-8ac865e3-21f9-4f1d-ba61-97d69a976284",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "42d5ebeb",
    "execution_start": 1623324653961,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "def nll( g,target):\n    return -g[range(target.shape[0]), target].mean()\n    \n\nloss_func = nll",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00019-6107d9a5-84de-4cb0-8724-2e23f78ddd31",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "995f4cdc",
    "execution_start": 1623324653963,
    "execution_millis": 6,
    "output_cleared": false,
    "deepnote_cell_type": "code"
   },
   "source": "yb = y_train[0:bs]\nprint(loss_func(preds,yb))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "tensor(2.3834, grad_fn=<NegBackward>)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Accuracy",
   "metadata": {
    "tags": [],
    "cell_id": "00021-18555134-29e9-4a6f-8eb1-0e73e8a8ce0a",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "1. Pick the index with largest probabilty for an instance from preds.\n2. Compare if the index matches with the ouput in the target array for same instance.\n3. This comparison returns a tensor with boolean values for matches and non-matches. Converting to float and then taking mean gives the accuracy for this random model.",
   "metadata": {
    "tags": [],
    "cell_id": "00023-721e563f-98d3-4a23-bb2c-6bdd2aa8736f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00022-176efac0-a117-401f-90c8-f6859c35893f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5acfb157",
    "execution_start": 1623324794839,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "def accuracy(out,yb):\n\n    preds = torch.argmax(out,dim=1)\n    \n    return (preds == yb).float().mean()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00024-7b9f764d-fc26-46af-bfa6-e78818e4239b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9b69f886",
    "execution_start": 1623324796329,
    "execution_millis": 16,
    "deepnote_cell_type": "code"
   },
   "source": "print(accuracy(preds, yb))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "tensor(0.0312)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Minimal neural network",
   "metadata": {
    "tags": [],
    "cell_id": "00025-e2686b67-7699-45ae-8597-d410573d9dd6",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "1. W and b are tensors with gradients enabled. Hence any function or variable that use these tensors will have backward() enabled.\n2. https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944/2 (It computes gradients of calling tensor w.r.t all the variables with grad = True)\n3. Each tensor T with grad enabled will have it's gradient computed in previous step accesible by using (T.grad)\n4. We then update these tensors by using above grad value and learning rate.\n5. Lastly , clear the value so that grad value doesn't add up after each pass.",
   "metadata": {
    "tags": [],
    "cell_id": "00026-035bba40-cec7-4fcc-99c9-254706a8a1ee",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00025-3e41e7e1-0776-4231-ae76-cacf71c09130",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ffae082e",
    "execution_start": 1623324822838,
    "execution_millis": 1226,
    "deepnote_cell_type": "code"
   },
   "source": "lr = 0.5\nepochs = 2\n\nfor epoch in range(epochs):\n    for i in range((n-1) // bs + 1):\n\n        start_i = i*bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred,yb)\n\n        loss.backward()\n        with torch.no_grad():\n            weights -= weights.grad * lr\n            bias -= bias.grad * lr\n            weights.grad.zero_()\n            bias.grad.zero_()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00027-a6514458-60e8-41e4-a0ea-8d89b3f40489",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e172f463",
    "execution_start": 1623324844229,
    "execution_millis": 13,
    "deepnote_cell_type": "code"
   },
   "source": "print(loss_func(model(xb), yb), accuracy(model(xb), yb))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "tensor(0.0801, grad_fn=<NegBackward>) tensor(1.)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00028-d70d8e39-92c0-4745-92f5-2bca0209a7f9",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a5eebfae-950c-452c-bcd1-02b41e6a5850' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "5135f93d-c573-4683-a40a-b664fa655b7f",
  "deepnote_execution_queue": []
 }
}